# Neural networks, deep learning papers

- **Overview**
  - [Deep Learning](http://www.deeplearningbook.org/) (2016) *Ian Goodfellow, Yoshua Bengio, Aaron Courville*
  - [Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828.pdf) (2014) *Jurgen Schmidhuber*

### Feedforward Neural Networks (FNN)
  - [The perceptron: a probabilistic model for information storage and organization in the brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958) *F. Rosenblatt*
  - [Multilayer Feedforward Networks are Universal Approximators](http://cognitivemedium.com/magic_paper/assets/Hornik.pdf) (1989) *K. Hornik*
  - [Deep Big Simple Neural Nets Excel on Hand-written Digit Recognition](https://arxiv.org/pdf/1003.0358.pdf) (2010) *Dan Claudiu Cireşan, Ueli Meier, Luca Maria Gambardella, Jürgen Schmidhuber*
- **GMDH** Group method of data handling ([Website](http://gmdh.net/), [Wiki](https://en.wikipedia.org/wiki/Group_method_of_data_handling))
  - [Polynomial Theory of Complex Systems](http://www.gmdh.net/articles/history/polynomial.pdf) (1971) *Ivakhnenko A.G.*
  - [The Review of Problems Solvable by Algorithms of the Group Method of Data Handling](http://gmdh.net/articles/review/algorith.pdf) (1995) *Ivakhnenko A.G., Ivakhnenko G.A.*
- **Binarized Neural Networks**
  - [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830) (2016) *Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio*
  - [How to Train a Compact Binary Neural Network with High Accuracy?](https://www.ganghua.org/publication/AAAI17.pdf) (2017) *Wei Tang, Gang Hua, Liang Wang*

### Convolutional Neural Networks (CNN)
  - One of the papers on convolutional nets - [Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position] (https://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf) (1980) *K. Fukushima*
  - [A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects](https://arxiv.org/pdf/2004.02806.pdf) *Zewen Li, Wenjie Yang, Shouheng Peng, Fan Liu*
  - [Flexible, High Performance ConvolutionalNeural Networks for Image Classification](http://people.idsia.ch/~juergen/ijcai2011.pdf) (2011) *Dan C. Ciresan, Ueli Meier, Jonathan Masci, Luca M. Gambardella, Jurgen Schmidhube*

### Recurrent Neural Networks (RNN)
- **Boltzmann machines**
  - [Learning and relearning in Boltzmann machines](https://www.researchgate.net/publication/242509302_Learning_and_relearning_in_Boltzmann_machines) (1986) *G. E. Hinton, T. J. Sejnowski*
- **LSTM**
  - [Long Short-term Memory](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) (1997) *S. Hochreiter, J. Schmidhuber*
  - [Framewise Phoneme Classification withBidirectional LSTM and Other Neural NetworkArchitectures](https://www.cs.toronto.edu/~graves/nn_2005.pdf) (2005) *Alex Graves, Jurgen Schmidhuber*

### Unsupervised
- **Competitive learning**
  - [Feature Discovery by Competitive Learning](http://csjarchive.cogsci.rpi.edu/1985v09/i01/p0075p0112/MAIN.PDF) (1985) *David E. Rumelhart*
- **Autoencoders**
  - [Modular learning in neural networks](https://www.aaai.org/Papers/AAAI/1987/AAAI87-050.pdf) (1987) *D.H. Ballard*
  - [Extracting and composing robust features with denoising autoencoders](https://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf) (2008) *P. Vincent, H. Larochelle, Y. Bengio, P.A. Manzagol*
  - From Deep Learning book - [Autoencoders (ch. 14)](http://www.deeplearningbook.org/contents/autoencoders.html) (2016) *Ian Goodfellow, Yoshua Bengio, Aaron Courville*
  - [An Introduction to Variational Autoencoders](https://arxiv.org/pdf/1906.02691.pdf) (2019) *Diederik P. Kingma, Max Welling*
  - [Contractive Auto-Encoders: Explicit Invariance During Feature Extraction](https://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf) (2011) *S. Rifai, P. Vincent, X. Muller, X. Glorot, Y. Bengio*
  - [Deep AutoRegressive Networks](https://arxiv.org/pdf/1310.8499.pdf) (2014) *Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan Wierstra*
- **Denoising Autoencoders**
- **VAE** Variational autoencoders
  - [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) (2014) *Diederik P Kingma, Max Welling*
  - [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf) (2016) *Carl Doersch*
  - [Variational Autoencoder for Deep Learning of Images, Labels and Captions](https://papers.nips.cc/paper/6528-variational-autoencoder-for-deep-learning-of-images-labels-and-captions.pdf) (2016) *Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, Lawrence Carin*
- **SOM** Self-organizing maps
- **Cresceptron** (Max-Pooling layers)
  - [Cresceptron: A Self-organizing  Neural Network Which Grows Adaptively](http://www.cse.msu.edu/~weng/research/CresceptronIJCNN1992.pdf) (1992) *John (Juyang) Weng, Narendra Ahuja, Thomas S. Huang*

### Generative Adversarial Networks (GAN)
- [Generative Adversarial Networks](https://arxiv.org/pdf/1406.2661v1.pdf) (2014) *Ian J. Goodfellow,  Jean Pouget-Abadie∗, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair†, Aaron Courville, Yoshua Bengio*
- [Time-series Generative Adversarial Networks](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks.pdf) (2019) *J. Yoon, D. Jarrett, M. van der Schaar*
- **Conditional GAN**
  - [Probabilistic Forecasting of Sensory Data with Generative Adversarial Networks](https://arxiv.org/abs/1903.12549) (2019) *A. Koochali, P. Schichtel, S. Ahmed, A. Dengel*

### Bayesian Neural Networks (BNN)
- [A Practical Bayesian Framework for Backpropagation Networks](https://authors.library.caltech.edu/13793/1/MACnc92b.pdf) (1992) *David J. C. MacKay*
- [Bayesian Learning for Neural Networks](http://www.csri.utoronto.ca/~radford/ftp/thesis.pdf) (1995) *R.M. Neal*
- [Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks](https://pdfs.semanticscholar.org/3ce9/da2d2182a2fbc4b460bdb56d3c34110b3e39.pdf) (1995) *David J. C. MacKay*
- [Practical Variational Inference for Neural Networks](https://www.cs.toronto.edu/~graves/nips_2011.pdf) (2011) *Alex Graves*
- [Weight Uncertainty in Neural Networks](https://arxiv.org/pdf/1505.05424.pdf) (2015) *Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra*
- [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/pdf/1506.02142.pdf) (2016) *Y. Gal, Z. Ghahramani*
- [Stochastic Gradient Descent as Approximate Bayesian Inference](http://www.cs.columbia.edu/~blei/papers/MandtHoffmanBlei2017.pdf) (2017) *S. Mandt, M.D. Hoffman, D.M. Blei*
- [Deep neural networks as Gaussian Processes](https://arxiv.org/pdf/1711.00165.pdf) (2018) *Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein*
- [Noisy Natural Gradient as Variational Inference](https://arxiv.org/pdf/1712.02390.pdf) (2018) *Guodong Zhang, Shengyang Sun, David Duvenaud, Roger Grosse*
- [Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam](https://arxiv.org/abs/1806.04854) (2018) *Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, Akash Srivastava*
- [Understanding Priors in Bayesian Neural Networks at the Unit Level](https://arxiv.org/pdf/1810.05193.pdf) (2019) *Mariia Vladimirova, Jakob Verbeek, Pablo Mesejo, Julyan Arbel*
- [Bayesian Deep Learning and a Probabilistic Perspective of Generalization](https://arxiv.org/pdf/2002.08791.pdf) (2020) *Andrew Gordon Wilson, Pavel Izmailov*

### Weightless Neural Networks (WNN)
- Based on Random Access Memory (RAM) nodes
- [Advances in Weightless Neural Systems](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2014-7.pdf) (2014) *F.M.G. França, M. De Gregorio, P.M.V. Lima, W.R. de Oliveira*
- **WiSARD** 
- **PLN** Probabilistic Logic Nodes
- **GSN** Goal Seeking Neurons
- **GRAM** 

### Activation functions
- **Sigmoid**
- **HardSigmoid**
- **SiLU, dSiLU**
- **Tanh, HardTanh**
- **Softmax**
- **Softplus**
- **Softsign**
- **ReLU** Rectified Linear Unit
  - [Rectified Linear Units Improve Restricted Boltzmann Machines](https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf) (2010) *V. Nair, G.E. Hinton*
  - [Deep Sparse Rectifier Neural Networks](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf) (2011) *X. Glorot, A. Bordes, Y. Bengio*
- **LReLU** Leaky ReLU
  - [Rectifier Nonlinearities Improve Neural Network Acoustic Models](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf) (2013) *A.L. Maas, A.Y. Hannun, A.Y. Ng*
- **PReLU** Parametric ReLU
  - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf) (2015) *Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun*
- **RReLU** Randomized ReLU
  - [Empirical Evaluation of Rectified Activations in Convolutional Network](https://arxiv.org/pdf/1505.00853.pdf) (2015) *Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li*
- **SReLU** 
- **ELU**
  - [Fast and Accurate Deep Network Learning by Exponential Linear Units](https://arxiv.org/pdf/1511.07289.pdf) (2015) *Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter*
- **PELU**
  - [Parametric Exponential Linear Unit forDeep Convolutional Neural Network](https://arxiv.org/pdf/1605.09332v1.pdf) (2016) *L. Trottier, P. Giguère, B. Chaib-draa*
- **SELU**
- **Maxout**
- **Mish**
  - [Mish: A Self Regularized Non-Monotonic Neural Activation Function](https://arxiv.org/pdf/1908.08681.pdf) (2019) *Diganta Misra*
- **Swish**
- **ELiSH**
- **HardELiSH**

### Inference
- **Weight guessing**
- **Vanishing gradient problem** ([Wiki](https://en.wikipedia.org/wiki/Vanishing_gradient_problem))
- **Double descent**
  - [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/pdf/1912.02292) (2019) *Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever*
- **BP** Back-propagation
  - [Learning representations by back-propagating errors](http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf) (1986) *David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams*
  - [Backpropagation Applied to Handwritten Zip Code Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) (1989) *Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel*
- **Pruning** - reduces computational cost, improves generalization
  - [Optimal Brain Damage](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) (1990) *Yann Le Cun, John S. Denker, Sara A. Solla*
  - [Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/pdf/1506.02626.pdf) (2015) *Song Han, Jeff Pool, John Tran, William J. Dally*
  - [Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/pdf/1611.06440.pdf) (2017) *Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz*
  - [Learning Sparse Neural Networks through L0 Regularization](https://arxiv.org/pdf/1712.01312.pdf) (2018) *Christos Louizos, Max Welling, Diederik P. Kingma*
- **Pretraining**
  - [Why Does Unsupervised Pre-training Help Deep Learning?](http://jmlr.csail.mit.edu/papers/volume11/erhan10a/erhan10a.pdf) (2010) *D. Erhan, Y. Bengio, A. Courville, P.A. Manzagol, P. Vincent, S. Bengio*
- **Dropout**
  - [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580.pdf) (2012) *G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. R. Salakhutdinov*
  - [Adaptive dropout for training deep neural networks](https://papers.nips.cc/paper/5032-adaptive-dropout-for-training-deep-neural-networks.pdf) (2013) *L.J. Ba, B. Frey*
  - [The Dropout Learning Algorithm](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3996711/pdf/nihms-570835.pdf) (2014) *P. Baldi, P. Sadowski*
  - [Fast dropout training](https://nlp.stanford.edu/pubs/sidaw13fast.pdf) (2013) *S.I. Wang, C.D. Manning*

### Compression
- **Knowledge Distillation**
  - Large neural networks (teacher networks) transfer knowledge to smaller networks (called student networks)
- **Neural Network Pruning**
  - Removing unimportant weights
- **Quantization**
  - Reducing the number of bits used to store the weights
- **Software**
  - [KD-Lib: A PyTorch library for Knowledge Distillation, Pruning and Quantization](https://arxiv.org/pdf/2011.14691.pdf) (2020) *Het Shah, Avishree Khare, Neelay Shah, Khizir Siddiqui*

### [Ensembles](https://github.com/mlpapers/ensemble-learning)
- [Neural Network Ensembles](http://machine-learning.martinsewell.com/ensembles/HansenSalamon1990.pdf) (1990) *L. K. Hansen, P. Salamon*
- [When Networks Disagree: Ensemble Methods for Hybrid Neural Networks](https://www.researchgate.net/publication/2438296_When_Networks_Disagree_Ensemble_Methods_for_Hybrid_Neural_Networks) (1993) *M.P. Perrone, L.N. Cooper*
- [Neural Network Ensembles, Cross Validation, and Active Learning](https://papers.nips.cc/paper/1001-neural-network-ensembles-cross-validation-and-active-learning.pdf) (1995) *A. Krogh, J. Vedelsby*
- [When Ensembling Smaller Models is More Efficient than SingleLarge Models](https://arxiv.org/pdf/2005.00570.pdf) (2020) *D. Kondratyuk, M. Tan, M. Brown, B. Gong*

### [Optimization](https://github.com/mlpapers/optimization)
